Kubernetes

start with kubernetes -

1. Create a kubernetes cluster on a cloud provider like digital ocean.
2. install kubectl on local macine 
3. download configuration yml file 
4. set path and save configurationfile in side this folder with .txt extention C:\Users\name\.kube
5. run kubectk on cmd to check 
6. run kubectl get nodes to check is kubernetes is connected with your local 
7. run kubectl run mywebserver --image= nginx to run a pod 
8. check pods using kubectl get pods 
9. get inside the pod usign exec -it Name -- bash

Kubernetes components 

---- etcd ----
etcd in kubernetes is a key value store database that stores all the information related to a cluster 

----Kube API server ----- 
++  https://kubernetes.io/ ++

- when we interact with the kubernetes cluster usign kubectl command line interface, we actually communicate 
  with master api server component
- all the other components of kubernetes must go threw kubernetes api server to work with the cluster state

-----Kube-scheduler----
Kube-scheduler watches for newly created pods that have no nods assigned, and selcet the nods for them to run on.

there are sevral factors  which are taken into considration before a pod is scheduled to anode.
 Resourec requirment 
 Hardware/software policy constrains 
 Affinity and Anti-Affinity
 Data locality 



************************YAML file to work with start a pod ***************************
*************************************************************************************
- newpod.yaml -
 
apiVersion: v1
kind: Pod 
metadata:
  name: nginx-webserver
spec:
  containers:
  - image: nginx
    name: demo-container
	

command to cretae a pod with yaml file 
>> kubectl apply -f newpod.yaml 

command to delete a pod with yaml file 
>> kubectl delete -f newpod.yaml 

---------------------------------------------Multi container pod -

file multi-containerpod.yaml 
apiVersion: v1
kind: Pod 
metadata:
  name: nginx-webserver
spec:
  containers:
  - image: nginx
    name: demo-container1
  - image: busybox
    name: demo-container2
    command:
    - sleep 
    - "3600"
	
create pod with yaml file >> kubectl apply -f newpod.yaml
 
then a pod with two container inside it will be created 

we can get it's description with command >> kubectl describe pod podname 


                                            get inside the pod -
kubectl exec -it nginx-webserver bash
with this command we can get inside a pod and that will use container1 as a default container 



---------------------------------------------------------------------------------------------------------------------
after installing net-tools we can see the ip used by container
>>apt-get update && apt-get install net-tools
>> netstat -ntlp

Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 0.0.0.0:80              0.0.0.0:*               LISTEN      1/nginx: master pro
tcp6       0      0 :::80                   :::*                    LISTEN      1/nginx: master pro


get inside a particular container within a pod 
                            
>>kubectl exec -it nginx-webserver -c demo-container2 sh

*****we can access container1 from this container because bote are in a same pod and sharing a same ip address 

{{{{{{{{{{{{{{{{{{{{{{Command and arguments _

in kubernetes we can override defauld CMD and ENTRYPOINT of docker image with the help of command and args field

this is a busybox docker file available on the docker hub 

// docker file  >>

FROM scratch
ADD busybox.tar.gz /
CMD ["sh"]

it has sh commad that exits the container after it creats

so when we crete a pod with >> kubectl apply -f busybox.yaml
and >> kubectl get pods 
then we get somthing like this 
NAME              READY   STATUS             RESTARTS       AGE
command           0/1     CrashLoopBackOff   1 (3s ago)     8s

becase sh command executed and container is exited 
 
but we can override this sh command present in the docker file with the help of command and args 
// busybox.yaml >>

apiVersion: v1
kind: Pod 
metadata:
  name: command1
spec:
  containers: 
  - name: busybox1
    image: busybox
    command: ["sleep"]
    args: ["34556"]

afer this status would be >>
NAME              READY   STATUS             RESTARTS        AGE
command1          1/1     Running            0               5s

>> if nothing is set from k8s level then docker Entrypoint will be final 
>> if somthing is in command of YAML and somthing in docker file then resuld will be mix of both

{{{{{{{{{{[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[CLI documentation 
kubectl explain component of kubernetes ex>> kubectl explain pod

we can get every component extention
>> explain pod.spec.containers.ports
>>explain pod.spec.containers
>>explain pod.spec

create pods with cli -----------------------------------

>>kubectl run nginx --image nginx 

define pod
>>kubectl run nginxp --port=80 --image=nginx

genrate yaml file and make dry run active 
>> kubectl run nginxpod --port=80 --image=nginx --dry-run=client -o yaml 



++++++++++++=======================Lables and selectors _____=================

Lables : Lables are the key value pairs that are attached to the kubernetes objects, suvh as pods.
Selectors : Selectors are allow us to filter objects based on lables.

LABELS -

first create pod > kubectl run pod1 --image=nginx

check labels > kubectl get pods --show-lables
-
NAME   READY   STATUS    RESTARTS   AGE   LABELS
pod1   1/1     Running   0          82s   run=pod1
pod2   1/1     Running   0          72s   run=pod2

add labels >   kubectl label pod pod1 env=Prd  (key value pair)

check labels > kubectl get pods --show-lables
-
NAME   READY   STATUS    RESTARTS   AGE     LABELS
pod1   1/1     Running   0          5m17s   env=Prd,run=pod1
pod2   1/1     Running   0          5m7s    env=UAT,run=pod2


Selectors - 

If I want to see the pods with particular labels
then I can run the command 
> get pods -l env=prd 
output 
pod3   1/1     Running   0          21m

if I don;t want pod with perticular label 
then I can a command > kubectl label pod pod1 env!=Prd
NAME   READY   STATUS    RESTARTS   AGE     LABELS
pod2   1/1     Running   0          5m7s    env=UAT,run=pod2


to remove Label >>>> kubectl label pod pod1 env-
 pod/pod1 unlabeled

to label all the pods at once 
>>kubectl label pod --all status=paid

>> get pods --show-labels
NAME     READY   STATUS    RESTARTS   AGE     LABELS
nginx1   1/1     Running   0          4m59s   env=dev,run=nginx1,status=paid
pod1     1/1     Running   0          47m     run=pod1,status=paid
pod2     1/1     Running   0          46m     env=UAT,run=pod2,status=paid
pod3     1/1     Running   0          46m     env=dev1,run=pod3,status=paid


---------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------
                           Replica set 
                           
A Replica Set purpose is to maintain a stable set of replica pods runiign at any given time 

Create a replicaset ReplicaSet.YAML

apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: ksreplicaset
spec:
  replicas: 5
  selector:
    matchLabels: ----lable under this should match with label of pod----------|
      tier: frontend                                                          |
  template: --------------pod starts from here                                |
    metadata:                                                                 |
      labels: ---------label of pod-------------------------------------------|
        tier: frontend
    spec:
      containers: 
      - name: phpapp
        image: nginx
        
------------------------ a replica set alwasy identify it's pods with their LABELS

----------------------------------------------------------------------------------------------------------------
                                   -  Deployments -
                                   
                                   
   Replicaset works well in basic functionality like manging pods, scaling pods and similar.. 
   
   but if some advanced functionalities are needed then deploymets are good choise 
  
 Deployments provides replication functionality with help of Replicasets, along with various additional capabilitity
 like rolling out cahnges, roling back cahnges  if required 
 
 Create a deployment >> Deplyments.yaml 
 
apiVersion: apps/v1           
kind: Deployment 
metadata:
  name: ksdeployments
spec:
  replicas: 5              -----------Deployments works over the replicaset 
  selector:
    matchLabels:
      tier: frontend
  template:
    metadata:
      labels:
        tier: frontend
    spec:
      containers:
      - name: phpapp
        image: nginx:1.17.3
 
 to create deployment>>>kubectl apply -f Deplyments.yaml
 
 deployment will be created>> kubectl get deployments 
 
NAME            READY   UP-TO-DATE   AVAILABLE   AGE
ksdeployments   5/5     5            5           33m

deployments manges replicasets so if we check >> kubectl get rs -

NAME                       DESIRED   CURRENT   READY   AGE
ksdeployments-855ddb4bc8   5         5         5       31m

if we make any change in our YAML file and create deployment again then new deployment will be created and 



============================================== Rollout Deployment
>kubectl rollout history deployment
deployment.apps/ksdeployments
REVISION  CHANGE-CAUSE
1         <none>
2         <none>

>>>>>>>>>kubectl rollout history deployment --revision 2   ----check REVISION
deployment.apps/ksdeployments with revision #2
Pod Template:
  Labels:       pod-template-hash=855ddb4bc8
        tier=frontend
  Containers:
   phpapp:
    Image:      nginx:1.17.3
    Port:       <none>
    Host Port:  <none>
    Environment:        <none>
    Mounts:     <none>
  Volumes:      <none>
  Node-Selectors:       <none>
  Tolerations:  <none>
  
  
>>>>>>>>kubectl rollout history deployment --revision 2
deployment.apps/ksdeployments with revision #2
Pod Template:
  Labels:       pod-template-hash=85747c8c88
        tier=frontend
  Containers:
   phpapp:
    Image:      nginx
    Port:       <none>
    Host Port:  <none>
    Environment:        <none>
    Mounts:     <none>
  Volumes:      <none>
  Node-Selectors:       <none>
  Tolerations:  <none>



  
  
=====================================--------Rollingback deployment------------
check replicaset -



>kubectl get rs

NAME                       DESIRED   CURRENT   READY   AGE
ksdeployments-855ddb4bc8   0         0         0       31m
ksdeployments-85747c8c88   5         5         5       31m


>>>kubectl rollout undo deployments ksdeployments --to-revision 1

deployment.apps/ksdeployments rolled back

after rolling back check replicaset -
 
>kubectl get rs

NAME                       DESIRED   CURRENT   READY   AGE
ksdeployments-855ddb4bc8   5         5         5       50m
ksdeployments-85747c8c88   0         0         0       50m
 now this is our revision 3
 
 
----------------------------------------Create deployment in CLI ------------------------------------------------------


>>>>>>>>>>>>>>>>>kubectl create deployment my-deployment --image=nginx --dry-run=client -o yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: my-deployment
  name: my-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: my-deployment
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: my-deployment
    spec:
      containers:
      - image: nginx
        name: nginx
        resources: {}
status: {}

and when we want multiple replicas 

>>>>>>>>>>>>>kubectl create deployment my-deployment --image=nginx --replicas 5 --dry-run=client -o yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: my-deployment
  name: my-deployment
spec:
  replicas: 5
  selector:
    matchLabels:
      app: my-deployment
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: my-deployment
    spec:
      containers:
      - image: nginx
        name: nginx
        resources: {}
status: {}

-----------change the image in deployment in case if may mistake is done or---


>>>>>>>>>>>>>>>kubectl set image deployment [deployment name] [container name]=nginx:latest < image 
check rollout history >
deployment.apps/ksdeployments
REVISION  CHANGE-CAUSE
2         <none>
3         <none>
4         <none>

>>>kubectl set image deployment ksdeployments phpapp=nginx:latest --record  (--record ) to keep the record of this command 

deployment.apps/ksdeployments image updated

C:\Users\shiva\Desktop\K8s>kubectl rollout history deployment ksdeployments
deployment.apps/ksdeployments
REVISION  CHANGE-CAUSE
2         <none>
3         <none>
4         kubectl set image deployment ksdeployments phpapp=nginx:latest --record=true



scale deployment================================

kubectl deployments >>

>>>>>>>>>>>>>>>>>>>>>>>> kubectl scale deployments ksdeployments --replicas 4

kubectl create deployment newdplmnt --image=nginex:latest --dry-run=client -o yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: newdplmnt
  name: newdplmnt
spec:
  replicas: 1
  selector:
    matchLabels:
      app: newdplmnt
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: newdplmnt
    spec:
      containers:
      - image: nginex:latest
        name: nginex
        resources: {}
status: {}



==================================================================================================
                                        Daemon set
Deamonset can insures that all the nodes run a copy of a pod 
as nodes are added to cluster pods are added to them 

as we can see we have 2 nodes in our cluster 
K8s>kubectl get nodes
NAME                   STATUS   ROLES    AGE    VERSION
pool-topyac6vi-r9hop   Ready    <none>   20d    v1.30.2
pool-v2lf9yhgf-brajw   Ready    <none>   3m4s   v1.30.2


if we create a deployment then all the pods may or my not me equally assign among all the nodes present in the current cluster 
 to solve this problem daemon set is there 
 
 ///////////////////////DaemonSet.YAML
 apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: ksdaemonset
spec:
  replicas: 5
  selector:
    matchLabels:
      tier: frontend
  template:
    metadata:
      labels:
        tier: frontend
    spec:
      containers:
      - name: phpapp
        image: nginx


K8s>kubectl apply -f Daemonset.yaml
daemonset.apps/ksdaemonset created


kubectl get pods -o wide                                              here we can see both pods are launched on different nodes
NAME                READY   STATUS    RESTARTS   AGE   IP             NODE                   NOMINATED NODE   READINESS GATES
ksdaemonset-8lwnr   1/1     Running   0          30s   10.244.0.70    pool-topyac6vi-r9hop   <none>           <none>
ksdaemonset-rxpgr   1/1     Running   0          30s   10.244.0.223   pool-v2lf9yhgf-brajw   <none>           <none>

if any new worker node will be created then new pod will automatically launched for that NODE



========================================================================================================

                                             Node Selector 
                                             
 node selector allow us to run constrains about runnign a pod in a specific worker node  

to label a node >>>> kubectl label node pool-topyac6vi-r9hop disk=ssd

if we want to launch a pod on this particular node we can add a node selector under teh spec section 
apiVersion: v1
kind: Pod
metadata:
  name: nginx-webserver
spec:
  containers:
  - image: nginx
    name: demo-container
  nodeSelector:
    disk: ssd    ---- here we can mention the label that node have 
    
then pode created from this file will be created in the node with same label 

====================================================================================================

                                        Node Affinity
       
       List the nodes in your cluster, along with their labels:

kubectl get nodes --show-labels

Choose one of your nodes, and add a label to it:

kubectl label nodes <your-node-name> disktype=ssd

Schedule a Pod using required node affinity 
This manifest describes a Pod that has a requiredDuringSchedulingIgnoredDuringExecution node affinity,disktype: ssd. This means that the pod will get scheduled only on a node that has a disktype=ssd label.


Schedule a Pod using preferred node affinity
This manifest describes a Pod that has a preferredDuringSchedulingIgnoredDuringExecution node affinity,disktype: ssd. This means that the pod will prefer a node that has a disktype=ssd label.
    
===================================================================================================================
                                        Resource Limit 
  if we Schedule a large application in a node which has limited resources, then it will soon led to OOM out of memmory 
  exeption.
  
  to define the resource limit of a pod we uses Request and Limit in the pod 
  Request is the inimum capacity of node to handel the application 
  Limit is the maximum capcity to handel the application 
  
apiVersion: v1
kind: Pod
metadata:
  name: nginx-webserver-rl
spec:
  containers:
  - image: nginx
    name: demo
    resources:
      requests:
        memory: "64Mi"
        cpu: "0"
      limits:
        memory: "128Mi"
        cpu: "0.5"

scheduler checks and run the pod on the node with matching criteria 
if no node is available then pod will not run
========================================================================================================


                Taints and Tolerations
                
                
  Taints are used to repel the pods from specific nodes -
  
  create taint
K8s>>>kubectl taint nodes pool-v2lf9yhgf-brajw key=value:NoSchedule

node/pool-v2lf9yhgf-brajw tainted

>>> kubectl describe node pool-v2lf9yhgf-brajw

CreationTimestamp:  Tue, 06 Aug 2024 16:00:19 +0530
---taints:             key=value:noschedule
unschedulable:      false
Lease:
  HolderIdentity:  pool-v2lf9yhgf-brajw
  AcquireTime:     <unset>

now taint is created for this specific node and any pod can not run on this 
if we create 5 pods in replicaset we can see 5 of them are runnign only on node without taint
kubectl get pods -o wide
NAME                             READY   STATUS    RESTARTS   AGE   IP            NODE                   NOMINATED NODE   READINESS GATES
my-deployment-7c86cc47f6-2fzjp   1/1     Running   0          24s   10.244.0.23   pool-topyac6vi-r9hop   <none>           <none>
my-deployment-7c86cc47f6-5kvvl   1/1     Running   0          24s   10.244.0.46   pool-topyac6vi-r9hop   <none>           <none>
my-deployment-7c86cc47f6-lx4dj   1/1     Running   0          24s   10.244.0.25   pool-topyac6vi-r9hop   <none>           <none>
my-deployment-7c86cc47f6-nqbhh   1/1     Running   0          24s   10.244.0.11   pool-topyac6vi-r9hop   <none>           <none>
my-deployment-7c86cc47f6-vg9l7   1/1     Running   0          24s   10.244.0.78   pool-topyac6vi-r9hop   <none>           <none>

if run pod on the taintednode we have to create a toleration 

apiVersion: v1
kind: Pod
metadata:
  name: nginx-webserver
spec:
  containers:
  - image: nginx
    name: demo-container
  tolerations:
  - key: "key"
    operator: "Exists"
    effect: "NoSchedule"
    
and after creating pod from this file we can see    

K8s>kubectl get pods -o wide
NAME                             READY   STATUS    RESTARTS   AGE     IP             NODE                   NOMINATED NODE   READINESS GATES
my-deployment-7c86cc47f6-2fzjp   1/1     Running   0          8m30s   10.244.0.23    pool-topyac6vi-r9hop   <none>           <none>
my-deployment-7c86cc47f6-5kvvl   1/1     Running   0          8m30s   10.244.0.46    pool-topyac6vi-r9hop   <none>           <none>
my-deployment-7c86cc47f6-lx4dj   1/1     Running   0          8m30s   10.244.0.25    pool-topyac6vi-r9hop   <none>           <none>
my-deployment-7c86cc47f6-nqbhh   1/1     Running   0          8m30s   10.244.0.11    pool-topyac6vi-r9hop   <none>           <none>
my-deployment-7c86cc47f6-vg9l7   1/1     Running   0          8m30s   10.244.0.78    pool-topyac6vi-r9hop   <none>           <none>
nginx-webserver                  1/1     Running   0          5s      10.244.0.241   pool-v2lf9yhgf-brajw   <none>           <none>

last pod is runnign on tainted NODE

 ==========================================================================
                                           Adapter Container pattern      
  At its core, the Adapter Container pattern involves deploying an additional container within the same pod as 
  your main application container. This secondary container acts as an intermediary, adapting or modifying data
  or communication between your main application and external systems. The main goal is to ensure seamless 
  integration without burdening your main application with protocol translation, data format conversion, 
  or encryption.
  
  
===============================================================================================================

                                        -----Services-----
                                       
whenever we creat a pod, container created will have a private IP address 

Kubernetes service acts as an abstraction which can provide single Ip address and DNS throw which pod can be accessed 

this layer of abstractionallow us to perform lot of operations like load balancing, scaling of pods and others

Kubernetes services -
- NodePort 
- Cluster IP 
- LoadBalancer 
- External Name 

Creating service endpoint in Kubernetes

Service and endpoints 

Service is a gateway that distributes incoming traffic between its endpoints
endpoints are underlying pods to which traffic will routed to

-----------------------------------------------------------------
there will be two backend pods and 1 front end pod and a backend gateway service

NAME                             READY   STATUS    RESTARTS   AGE
backend-pod-1                    1/1     Running   0          106s
backend-pod-2                    1/1     Running   0          102s
frontend-pod                     1/1     Running   0          8s


apt-get update && apt-get -y install curl
curl 10.244.0.19 
result - 
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
html { color-scheme: light dark; }
body { width: 35em; margin: 0 auto;
font-family: Tahoma, Verdana, Arial, sans-serif; }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>

------------------------------

create service -

apiVersion: v1
kind: Service
metadata:
   name: kplabs-service
spec:
   ports:
   - port: 8080
     targetPort: 80
     
 
 create endpoint - 
 
apiVersion: v1
kind: Endpoints
metadata:
  name: kplabs-service
subsets:
  - addresses:
      - ip: 10.244.0.23
    ports:
      - port: 80
      
 
 
kubectl exec -it frontend-pod -- bash
curl <SERVICE-IP:8080>

----------------------------------------------------------------------

                                    Service Selector
                                    
                                            
Services use labels and selectors to define which Pods are part of the service. 
Labels are key-value pairs attached to Pods, while selectors are used by services to filter and
include Pods matching those labels.

create a Deployment 

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80


This Deployment has 3 replicas with "nginx" label 


then create a service 

apiVersion: v1
kind: Service
metadata:
   name: my-service-selector
spec:
   selector:
     app: nginx   ---------------------selector with matching lable 
   ports:
   - port: 80
     targetPort: 80
     
then run this command - 
     
>>>>kubectl describe service my-service-selector


Name:              my-service-selector
Namespace:         default
Labels:            <none>
Annotations:       <none>
Selector:          app=nginx
Type:              ClusterIP
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:                10.245.208.99
IPs:               10.245.208.99
Port:              <unset>  80/TCP
TargetPort:        80/TCP
Endpoints:         10.244.0.101:80,10.244.0.26:80,10.244.0.28:80    all 3 IPS are here 
Session Affinity:  None
Events:            <none>
 

and when we scalte the replicas 
>> kubectl scale deployment/nginx-deployment --replicas=10
 
 
and check >kubectl describe service my-service-selector
 
Name:              my-service-selector
Namespace:         default
Labels:            <none>
Annotations:       <none>
Selector:          app=nginx
Type:              ClusterIP
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:                10.245.208.99
IPs:               10.245.208.99
Port:              <unset>  80/TCP
TargetPort:        80/TCP
Endpoints:         10.244.0.101:80,10.244.0.24:80,10.244.0.26:80 + 7 more...   ---- we can see 7 new IP are added automatically 
Session Affinity:  None
Events:            <none>


so this is how we don't need to add all the ips of our pods to the srvice manually 

==========================================================================================================
                                             NodePort


Node port exposes the service on each nods's IP at a static port(Node Port)

You'll be able to contact the NodePort Service, from outside the cluster, by requesting
<WorkerIP>:<NodePort>.
If servicetype is NodePort, then Kubernetes will allocate a port (default: 30000-32767) on every
worker node.

create a pod 

>kubectl run nodeport-pod --labels="type=publicpod" --image=nginx

Create a Node-port service 
/nodeport

apiVersion: v1
kind: Service
metadata:
  name: my-nodeport
spec:
  selector:
    type: publicpod
  type: NodePort
  ports:
  - port: 80
    targetPort: 80
    
 > kubectl get service 
 NAME                  TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
kubernetes            ClusterIP   10.245.0.1       <none>        443/TCP        31d
my-nodeport           NodePort    10.245.226.75    <none>        80:31696/TCP   3m5s
my-service            ClusterIP   10.245.183.108   <none>        8080/TCP       5d23h
my-service-selector   ClusterIP   10.245.208.99    <none>        80/TCP         47h

here we can see in nodeport service, there is one more port 31696 along with port 80 this is port of our node 

and by using this port along with nods IP we can access our pod from internet http://64.227.129.77:31696/

================================================================================================================


                                      Load Balancer service 
                                                                                         
                                            
 when we use node port evertime we have to always mention port number with IP or DNS 

 Load balancer service type will automatically deploy an external load balancer 
 this load balancer takes care of routing request to the underlying service.
 
 implement - 
 
 create a pod 
 >kubectl run lb-pod --labels="type=loadbalanced" --image=nginx

Create a load balancer service 
apiVersion: v1
kind: Service
metadata:
  name: my-loadbalancer
spec:
  type: LoadBalancer
  ports:
  - port: 80
    protocol: TCP
  selector:
    type: loadbalanced

>kubectl apply -f elb-service.yaml

when we run this command en external load balancer on the cloud provider is getting created 

C:\Users\shiva\Desktop\K8s\Kubernetes\Networking>kubectl get service
NAME                  TYPE           CLUSTER-IP       EXTERNAL-IP      PORT(S)        AGE
kubernetes            ClusterIP      10.245.0.1       <none>           443/TCP        32d
my-loadbalancer       LoadBalancer   10.245.77.141    144.126.255.23   80:30686/TCP   3m11s --- here we can see an external IP address
my-nodeport           NodePort       10.245.226.75    <none>           80:31696/TCP   19h
my-service            ClusterIP      10.245.183.108   <none>           8080/TCP       6d19h
my-service-selector   ClusterIP      10.245.208.99    <none>           80/TCP         2d19h

we can access this pod with ip 144.126.255.23 


=======================================================================================================

                                 Menifest service with CLI 
 
first create a pod with name nginx 

kubectl run nginx image=nginx

cli command to create service for and add pod to it 

                               (service name(new))
>> kubectl expose pod nginx --name nginx-svc --port=80 --target-port=80 --dry-run=client -o yaml 
                (pod name(existing))
                    
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: null
  labels:
    run: nginx
  name: nginx-svc
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 80
  selector:
    run: nginx
status:
  loadBalancer: {}
  
  
we can do modifications in the file after saving it
                                                                                                (file name)
>> kubectl expose pod nginx --name nginx-svc --port=80 --target-port=80 --dry-run=client -o yaml > service.yaml  
  
  
define type of service 

>> kubectl expose pod nginx --name nginx-svc-nodeport --port=80 --target-port=80 --type=NodePort --dry -run=client -o yaml
                                                                              (type of service)
  
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: null
  labels:
    run: nginx
  name: nginx-svc-nodeport
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 80
  selector:
    run: nginx
  type: NodePort ------------(type)
status:
  loadBalancer: {}

expose deployment 
 
>>kubectl expose deployment nginx-deployment --name deploymentsvc --port=80 --target-port=80 --dry-run=client -o yaml

apiVersion: v1
kind: Service
metadata:
  creationTimestamp: null
  labels:
    app: nginx
  name: deploymentsvc
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 80
  selector:
    app: nginx
status:
  loadBalancer: {}
  
We had label "nginx" in our deployment so here our service automatcally added a selector "nginx" 


=====================================================================================================================

                                         Kubernetes Ingress
                                         
    Kubernetes ingress is a collection of routing rules which governs how External user access the service running within the
    Kubernetes cluster 
    
    ingress can provide various features which includes 
    load balancing 
    SSL Termination 
    Named-based virtual hosting 
    
=================================================================================================
                                              \ --|-- /
                                            --- Helm ---
                                              / --|-- \
      Helm is one of the package manager for kubernetes 
      
      it contains all of the resource definitions necessary to run an application, tool or service inside of a kubernetes cluster 
      
      a repository is a place where charts can be collected and shared
      
      helm packages can be places in the local disk or can also be stored in public or private repository 
      
      
      when istall helm package from repository then we need to mention repo name
      
      Jenkins ---
      >> helm repo add jenkins https://charts.jenkins.io   --    when package is present in any repositpry 
      
      "jenkins" has been added to your repositories
      
      >> helm repo update
      
      >> helm install my-jenkins jenkins/jenkins
      
      Woerdpress ---
      >> helm repo add bitnami https://charts.bitnami.com/bitnami
      >> helm install my-wordpress bitnami/wordpress
      
      
=========================================================================================

                                            Ingress Example 
                                            
                                            
create pods pod1 pod2 

expose pods with service service1 and service2  

create a frontend pod                                         
                                            
  
create file ingress.yaml

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress-wildcard-host   ---------------  ingress name 
spec:
  ingressClassName: nginx      
  rules:
  - host: "web.website01.internal"       ------------- ip 
    http:
      paths:
      - pathType: Prefix
        path: "/bar"
        backend:
          service:
            name: service1          --------------service name 
            port:
              number: 80
  - host: "web.website02.internal"
    http:
      paths:
      - pathType: Prefix
        path: "/foo"
        backend:
          service:
            name: service2
            port:
              number: 80
              
kubectl apply -f ingress.yaml  


>kubectl get ingress
NAME                    CLASS   HOSTS                                           ADDRESS         PORTS   AGE
ingress-wildcard-host   nginx   web.website01.internal,web.website02.internal   68.183.246.43   80      41h


>kubectl describe ingress ingress-wildcard-host
Name:             ingress-wildcard-host
Labels:           <none>
Namespace:        default
Address:          68.183.246.43
Ingress Class:    nginx
Default backend:  <default>
Rules:
  Host                    Path  Backends
  ----                    ----  --------
  web.website01.internal
                          /bar   service1:80 (10.244.0.120:80)
  web.website02.internal
                          /foo   service2:80 (10.244.0.120:80)
Annotations:              <none>
Events:                   <none>


after runnign this command we can see a load balancer will be created in the cloud provider

         
              
install ingress controller with helm

 helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
 helm install [RELEASE_NAME] ingress-nginx/ingress-nginx

>helm list --all-namespaces
NAME            NAMESPACE       REVISION        UPDATED                                 STATUS          CHART
        APP VERSION
ingress-nginx   default         1               2024-08-26 00:43:38.8848904 +0530 IST   deployed        ingress-nginx-4.11.2    1.11.2


>kubectl get ingressclass
NAME    CONTROLLER             PARAMETERS   AGE
nginx   k8s.io/ingress-nginx   <none>       40h


go inside the frontend pod and curl on both 

> kubectl exec -it frontendpod -- bash 
curl web.website02.internal
should get response 

curl web.website01.internal
should get response 

then unistall controller 
>helm uninstall ingress-nginx
release "ingress-nginx" uninstalled

================================================================================================================

                                            Kubernetes Namespace
                                            
                                            
Kubernetes supports multiple vertual clusters backed by the same physical cluster, these vertual clusters are called namespaces.
     
    
Example Use-Case:
You provide all the users within Team A full access on "Pods" resources [only for Team A NS]
You provide all the users within Team A full access on deployments [only for Team B NS]

>Kubectl get namespace

NAME              STATUS   AGE
default           Active   44d
kube-node-lease   Active   44d
kube-public       Active   44d
kube-system       Active   44d


if we run any command without specifying namespace then it executes on default Namespace

>Kubectl get pods --- default Namespace
NAME                                    READY   STATUS    RESTARTS       AGE
frontend-pod                            1/1     Running   12 (58m ago)   5d
my-release-mariadb-0                    0/1     Pending   0              2d23h
my-release-wordpress-5bbf78b7f5-trbcl   0/1     Pending   0              10d
my-wordpress-6f46677859-wm6nt           0/1     Pending   0              10d
my-wordpress-mariadb-0                  0/1     Pending   0              10d
.
.


>Kubectl get pods --namespace kube-system  ---- name space 
NAME                            READY   STATUS    RESTARTS   AGE
cilium-hzjpl                    1/1     Running   0          44d
cilium-k4zxd                    1/1     Running   0          24d
coredns-85f59d8784-67pgz        1/1     Running   0          2d23h
coredns-85f59d8784-mrr7j        1/1     Running   0          44d
cpc-bridge-proxy-29pmd          1/1     Running   0          44d
cpc-bridge-proxy-npklw          1/1     Running   0          24d
.
.


create Namespace----

kubectl create namespace {teama} namespace name 
namespace/teama created

>Kubectl get namespace
NAME              STATUS   AGE
default           Active   44d
kube-node-lease   Active   44d
kube-public       Active   44d
kube-system       Active   44d
teama             Active   94s --- newly created name sapce 
 
>Kubectl get pods --namespace teama
No resources found in teama namespace.


>Kubectl run nginx --image=nginx --namespace teama    --- crteate resource in new namespace 
pod/nginx created

>Kubectl get pods --namespace teama
NAME    READY   STATUS    RESTARTS   AGE
nginx   1/1     Running   0          11s


Namespace Description

default  - The default namespace for objects with no other namespace

kube-system -  The namespace for objects created by the Kubernetes system

kube-public -  This namespace is created automatically and is readable by all users. It
contains information, like CA, that helps kubeadm join and authenticate
worker nodes.

kube-node-release -  The kube-node-lease namespace contains lease objects that are used
by kubelet to determine node health.


===================================================================================================

                                Service Accounts 
                                
                                
      Kubernetes clusters have two kinds of Accounts
      
      User Accounts (for Humans)
      Service Accounts (for Application)


        if any user or any kind of service wants to perform an opration on kubernetes cluster they need some kind of authentication 
        it can be a token(kubeconfig file) or it can be a certificate or so on 
        
        >kubectl exec -it app-pod bash
        
        TOKEN LOCATION INSIDE pod
        
        root@app-pod:/# cd /var/run/secrets/kubernetes.io/serviceaccount/
        root@app-pod:/var/run/secrets/kubernetes.io/serviceaccount# 1s
        ca.crt namespace toke
        root@app-pod:/var/run/secrets/kubernetes.io/serviceaccount# 
        
        
        
each namespcae have a service account named default 

>kubectl get ns
NAME              STATUS   AGE
default           Active   45d
kube-node-lease   Active   45d
kube-public       Active   45d
kube-system       Active   45d
team1             Active   23h
teama             Active   23h

>kubectl get sa -n teama
NAME      SECRETS   AGE
default   0         23h


each service account can be a associated with certain service accounts 

if we want a specific service account to be mounted to a pod so for that we can put that under the spec


spec:
   serviceAccountName: newserviceaccount
   
   
==============================================================================================================

                                    Named Port 
                                    
                                    
Insted of port number we can also specify port Name 

>kubectl run nginx --image=nginx --port=80 --dry-run=client -o yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: nginx
  name: nginx
spec:
  containers:
  - image: nginx
    name: nginx
    ports:
    - containerPort: 80
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}


>kubectl expose pod nginx --name first-svc --port=80 --target-port=http
--type=NodePort
service/first-svc exposed

C:\Users\shiva\Desktop\K8s\Kubernetes\Networking>kubectl get svc
NAME                   TYPE           CLUSTER-IP       EXTERNAL-IP      PORT(S)                      AGE
first-svc              NodePort       10.245.28.161    <none>           80:32457/TCP                 4s

>kubectl get svc first-svc -o wide
NAME        TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE   SELECTOR
first-svc   NodePort   10.245.28.161   <none>        80:32457/TCP   36s   run=nginx


>curl  139.59.28.42:32457
curl: (28) Failed to connect to 139.59.28.42 port 32457 after 21047 ms: Couldn't connect to server

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: nginx2
  name: nginx2
spec:
  containers:
  - image: nginx
    name: nginx2
    ports:
    - containerPort: 80
      name: custom-http
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}

 
 >curl  139.59.28.42:32611
<!DOCTYPE html>



>kubectl describe svc named-svc
Name:                     named-svc
Namespace:                default
Labels:                   run=nginx2
Annotations:              <none>
Selector:                 run=nginx2
Type:                     NodePort
IP Family Policy:         SingleStack
IP Families:              IPv4
IP:                       10.245.53.158
IPs:                      10.245.53.158
Port:                     <unset>  80/TCP
TargetPort:               custom-http/TCP
NodePort:                 <unset>  32611/TCP
Endpoints:                10.244.0.53:80
Session Affinity:         None
External Traffic Policy:  Cluster
Events:                   <none>





      
      
      
      
      
      
      
      
     
       
       
                                            
     







                                               
                                               
 
  
  
  
  
  
                                        
                                        


    
    
    
    
    
    

 

 
                                             
                                             












 
 
   























    




	



 

